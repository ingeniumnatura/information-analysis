

INFORMATION ANALYSIS


Joseph Shaw


Use and redistribution of this material with or without modification is permitted provided that the following conditions are met:

Redistributions of material and/or other materials provided in any format must retain the above notice that identifies and credits the original author and contributors, this list of conditions and the following disclaimers:


THIS MATERIAL IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS 'AS IS'.

ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.

IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, TORT, OR ANY CIVIL WRONG THAT CAUSES LOSS OR HARM (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS MATERIAL, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.


---

‚ÄúThe code is more what you'd call 'guidelines' than actual rules.‚Äù

---


=======  INTRODUCTION  =======



Information theory is an extremely powerful framework to deepen and enrich understanding across a broad variety of technical fields, from obvious applications in computer science such as data compression and encryption, to statistics and machine learning, to fundamental theories of physics. 

Such a broadly applicable study that lies at the intersection of many technical fields has resulted in some formulations that use arguably esoteric mathematical notation that may be difficult to understand for those new to the subject. Some authors dismiss this as expected convention, as it certainly is understandable that a such a rich subject that builds upon a broad set of different branches of mathematics requires some liberty in notation. This text is an attempt to develop information theoretic structures using a style of notation both accessible and appropriately robust; successes in fields like analysis are inspiration. 

Familiarity with basic concepts and structures of real analysis is recommended prerequisite material; set-builder notation is frequently used for discrete cases, and integrals and derivatives for continuous. 

As data compression is one of the core applications of information theory, emphasis is placed on developing definitions in a compact style. Mathematical logic symbols are frequently used, however, some liberty will occasionally be taken in notation; this is not an exercise in axiomatically describing structures in number theory or analysis or a rigorous formulation of mathematical logic. Rather, consider these foundations upon which this analytic development of information theory is built.



=======  NOTATION  =======



‚àÖ is the empty set. 

‚Ñï is the set of natural numbers beginning with 1 and excluding 0.[¬π]

‚Ñù is the set of real numbers.



Sets are constructed using set-builder notation:

	
	{expression | predicate}

		Expressions designate statements using numbers, variables, functions, etc.

		Predicates are logical formulas applied to expressions.

		The  |  symbol, read as 'such that', separates expressions from predicates.

		Simple finite sets are constructed without predicates. 

		For example the set containing the letters a, b, c, and d as elements is simply:
		
		{a, b, c, d}.
	


Logic symbols are used to construct formulas and expressions:


	‚à™ (union)	
		
		operation combines all unique elements of multiple sets:


			{a, b, c} ‚à™ {d, e, f} = {a, b, c, d, e, f}

			if A = {1, 2, 3} and B = {3, 4, 5}, then A ‚à™ B = {1, 2, 3, 4, 5}

			‚à™( {1, 2, 3}, {4, 5}, {6} ) = {1, 2, 3, 4, 5, 6}
 


	‚à© (intersection)	

		operation results in a set with all elements that are common to multiple sets:


			{0, 3, q, k} ‚à© {0, k, j, 340} = {0, k}

			if A = {1, 2, 3} and B = {3, 4, 5}, then A ‚à© B = {3}

			‚à©( {1, 2, 3}, {a, b, c}, {4} ) = ‚àÖ



	- (difference)

		operation, when applied to sets results in a set with elements that are in one set but not in another:


			{a, b, c} - {b, c} = {a}

			{0, 1, 2, 4} - {0, 1, 3, 4, 5} = {2}

			For any sets A and B, A - (A - B) = A ‚à© B 
	


	‚àà (is an element of)	

		indicates that an element belongs to a set:


			0 ‚àà {0, 1, 2, 3, 4}

			if A = {1, 2, 3}, then 3 ‚àà A



	‚äÜ (is a subset of)

		indicates that a set is either equivalent to or a subset of another set:


			for any set A, A ‚äÜ A

			{1, 2} ‚äÜ {0, 1, 2, 3}

			{‚àÖ} ‚äÜ {‚àÖ}

			{a, d, e} ‚äÜ {b, a, c, d, e}



	‚àÉ (there exists, there is at least one)
		
		is called the existential quantifier. For example, consider this statement:

			Some natural number n exists such that n minus 1 is equal to 9.

			This can be expressed:
				
			‚àÉ n ‚àà ‚Ñï | n - 1 = 9
		


	‚àÄ (for all, for each)

		expresses universal quantification, or that a statement holds true for all elements in a set:

			(x < 10) ‚àÄ x ‚àà {6, 7, 8, 9}
		

	
	‚áí (implies)

		expresses that a statement is necessarily true dependent upon if another is true; if the statement before the symbol is true, the truth of the second statement after the symbol is necessarily true:

		x + 2 = 4 ‚áí x = 2


	
	Œ£ (summation)
		
		adds a sequence of numbers:


			Œ£(i) ‚àÄ i ‚àà {0, 2, 4, 6} = 12

			or more compactly:

			Œ£{0, 2, 4, 6} = 12


		For brevity, for any set A consider Œ£A equivalent to Œ£(i) ‚àÄ i ‚àà A
	


	‚à´ (integration) 

		is the analog of summation for a continuous function. For example the definite integral:


		‚à´[a, b] f(x) dx


		represents the accumulation of values of the function f(x) over the continuous interval a ‚â§ x ‚â§ b.

		Here, dx represents an infinitesimal change in a variable x and to integrate with respect to x.





Logarithms:


	log‚Çô (logarithm base n):

		log‚Çô(x) is the exponent to which n must be raised to produce x. For example:

		8 = 2¬≥, so:

		log‚ÇÇ(8) = 3.


	ln (natural logarithm):
	
		is the logarithm with a base of Euler's number e = Œ£{1/(n - 1)! | n ‚àà ‚Ñï}:

		ln(eÀ£) = x






---

[¬π] Some definitions of ‚Ñï begin with and include 0 rather than beginning with 1:
https://www.iso.org/standard/76921.html

---


=======  œÉ-RINGS  =======



Probability spaces, foundational structures in probability and information theory, are specific examples of a general analytic structure: measure spaces.



A collection of sets ‚Ñõ is a œÉ-ring if: 


	the empty set is an element of ‚Ñõ:
	
	‚àÖ ‚àà ‚Ñõ,


	‚Ñõ is closed under union:

	(A ‚à™ B) ‚àà ‚Ñõ ‚àÄ A, B ‚àà ‚Ñõ,


	‚Ñõ is closed under difference[¬≤]:

	(A - B) ‚àà ‚Ñõ ‚àÄ A, B ‚àà ‚Ñõ, 	and


	‚Ñõ is closed under countable unions:

	‚à™{A‚Çô | n ‚àà ‚Ñï} ‚àà ‚Ñõ ‚àÄ A‚Çô ‚àà ‚Ñõ

	where A‚Çô is a countably infinite sequence of sets as implied by the condition n ‚àà ‚Ñï.



The operations of union and difference here are analogous to addition and subtraction in arithmetic; they extend the concept of closure to sets. Just as additive and subtractive closure is a necessary property of algebraic structures like the real numbers ‚Ñù (in example, addition or subtraction of any pair of real numbers results in another real number), closure under operations of union and difference are properties of œÉ-rings.



A few other important properties of a œÉ-ring ‚Ñõ are implied from the above definition:



	For any A‚Çô in ‚Ñõ, its compliment ‚Ñõ - A‚Çô will also be in ‚Ñõ:

		{‚Ñõ - A‚Çô | n ‚àà ‚Ñï} ‚àà ‚Ñõ ‚àÄ A‚Çô ‚àà ‚Ñõ.



	Because A - (A - B) = A ‚à© B, ‚Ñõ is also closed under intersection: 

		(A ‚à© B) ‚àà ‚Ñõ ‚àÄ A, B ‚àà ‚Ñõ.




œÉ-rings are often called œÉ-algebras and similar structures also labeled 'rings' are central to modern abstract algebra. In the modern abstract algebraic context, a ring is defined as a set with two binary operations where the second operation is distributive with respect to the first. The real numbers ‚Ñù are an example of an algebra with addition (+) and multiplication (¬∑) as binary operations; multiplication is distributive with respect to addition on ‚Ñù:
	
	
	‚àÄ a, b, c ‚àà ‚Ñù:

		a¬∑(b + c) = a¬∑b + b¬∑c

		and
	
		(b + c)¬∑a = b¬∑a + c¬∑a.


Likewise a œÉ-ring ‚Ñõ with binary operations of union (‚à™) and intersection (‚à©) also satisfy this property:


	‚àÄ A, B, C ‚àà ‚Ñõ:

		A ‚à© (B ‚à™ C) = (A ‚à© B) ‚à™ (B ‚à© C)

		and
	
		(B ‚à™ C) ‚à© A = (B ‚à© A) ‚à™ (C ‚à© A).


In an algebra such as ‚Ñù each element also necessarily has an inverse with respect to the additive binary operation (+):

	
	‚àÄ a ‚àà ‚Ñù ‚àÉ b ‚àà ‚Ñù | a + b = b + a = 0.


In a œÉ-ring, however, elements lack an analogous inverse with respect to the binary operation of union (‚à™); definitions of rings in the context of modern abstract algebra typically require them to be an abelian group under their additive binary operation and the absence of an inverse for each element of a œÉ-ring with respect to union thus distinguishes a œÉ-ring from an algebra.



---

[¬≤] Note that this definition of 'difference' is distinct from and not identical to 'symmetric difference' corresponding to XOR in boolean logic.

---



=======  MEASURE SPACES  =======



A measure space consists of three components (K, ùíú, Œº) where: 


	K is a set, 
    
	ùíú is a œÉ-ring constructed from a collection of subsets of K, and 

	Œº is a measure, a function with the following properties:


		Œº(‚àÖ) = 0,


		for any set A in ùíú, Œº(A) is non-negative:
		
		Œº(A) ‚â• 0 ‚àÄ A ‚àà ùíú, and


		Œº is countably additive:

       	Œº({A‚Çô | n ‚àà ‚Ñï}) = Œ£{Œº(A‚Çô) | n ‚àà ‚Ñï, A·µ¢ ‚à© A‚±º = ‚àÖ ‚àÄ i‚â†j}.

	

Œº is countably additive if for any countable sequence {A‚Çô} of pairwise disjoint sets in ùíú (sets with no common elements), the measure of their union is equal to the sum of their individual measures. Note that while A‚Çô is necessarily countable, the set K and œÉ-ring ùíú may be uncountable.



Henri Lebesgue is credited with formally introducing this concept of measure as a generalized notion of "size" or "extent" of a set. For another example, Lebesgue measures applied to 1, 2, and 3 dimensional Euclidean spaces are length, area, and volume respectively.[¬≥]


---

[¬≥] Remarques sur les th√©ories de la mesure et de l'int√©gration. Henri Lebesgue. Annales scientifiques de l'√âcole Normale Sup√©rieure, Serie 3, Volume 35, pages 191-250. Publi√© 1918.
https://doi.org/10.24033/asens.707
http://www.numdam.org/articles/10.24033/asens.707/

---


=======  PROBABILITY SPACES & RANDOM VARIABLES  =======



A probability space is a measure space consisting of (Œ©, ùìï, P) where: 


	Œ©, the sample space, is the set of all possible outcomes of a random experiment, 
    

	ùìï, the event space, is a a œÉ-ring consisting of a set of subsets of Œ© that represent events, 

	where an event is defined as a set of outcomes, and 


	P is a measure function that assigns probabilities to each event in the œÉ-ring ùìï such that P(Œ©) = 1.


Since P is a measure function, P(‚àÖ) = 0 and P is countably additive:


	P({A‚Çô | n ‚àà ‚Ñï}) = Œ£{P(A‚Çô) | n ‚àà ‚Ñï, A·µ¢ ‚à© A‚±º = ‚àÖ ‚àÄ i‚â†j}.


A typical event space ùìï is the power set of the sample space Œ©, or the collection of all possible subsets of Œ©, including ‚àÖ and Œ© itself where the power set of a set A, denoted 2·¥¨ is defined as:

	
	2·¥¨ = {S ‚à£ S ‚äÜ A}.


For a random variable X, the sample space Œ© is the set of all possible outcomes of x. 


	If X is discrete[‚Å¥]:

	Œ© = {x‚Çô | n ‚àà ‚Ñï}.


	Likewise, if X is continuous and ranges between a and b:
		
	Œ© = {x | a ‚â§ x ‚â§ b}.

		

P(x), the probability measure, is the probability that an outcome x will occur; the sum of probabilities of all outcomes is 1:



	Œ£{P(x‚Çô) | n ‚àà ‚Ñï} = P(Œ©) = 1

	if X is discrete. P is a countably additive measure function so:

	Œ£(P({x‚Çô | n ‚àà ‚Ñï})) = P(Œ©) = 1. 



	Likewise, if X is continuous:

	‚à´ P(x) dx = 1
		
	where the integral ranges over the entire domain of X.



To demonstrate, a trivial example of a probability space is used to quantify probabilities associated with possible outcomes of flipping a fair coin. After this random experiment, the coin can either land with the 'heads' side up or alternatively with the opposite 'tails' side facing up. The sample space Œ© is simply:


	Œ© = {heads, tails}.


An event space ùìï is constructed from the power set of Œ©; ùìï = {S ‚à£ S ‚äÜ Œ©}:

	
	ùìï = {‚àÖ, {heads}, {tails}, {heads, tails}}. 


Note that even in this simple example, ùìï satisfies all properties required of a œÉ-ring.


Now, the measure function P assigns a probability to each element of ùìï such that P(Œ©) = 1. Since the coin is fair, there is equal probability of each outcome:


	P({heads}) = P({tails}) = 0.5.


P is necessarily countably additive so:


	P({heads}) + P({tails}) = P(Œ©) = 1.

	

Another trivial example quantifies probabilities values associated with the possible outcomes of the random experiment of rolling a fair six-sided die:


	Œ© = {1, 2, 3, 4, 5, 6},

	ùìï = {S ‚à£ S ‚äÜ Œ©}, and
	
	P({1}) = P({2}) = P({3}) = P({4}) = P({5}) = P({6}) = 1/6.


There are 2‚Å∂ = 64 subsets in this event space:
	
	
	ùìï = 	{‚àÖ, 
		{1}, {2}, {3}, {4}, {5}, {6},
		{1, 2}, {1, 3}, ... , {4, 6}, {5, 6},
		{1, 2, 3}, {1, 2, 4}, ... , {3, 5, 6}, {4, 5, 6},
		{1, 2, 3, 4}, {1, 2, 3, 5}, ... , {2, 4, 5, 6}, {3, 4, 5, 6},
		{1, 2, 3, 4, 5}, {1, 2, 3, 4, 6}, ... , {2, 3, 4, 5, 6}, Œ©}.



---

[‚Å¥] Definitions of 'discrete' used in different mathematical contexts may vary slightly; here it refers to a random variable that can take on a countable set of distinct values. For a rigorous definition of a discrete set in a general topological space, see:
 
Discrete Set. Eric W Weisstein. MathWorld--A Wolfram Web Resource.
https://mathworld.wolfram.com/DiscreteSet.html

---

=======  EXPECTATION VALUES ‚Ñ∞  =======



The expectation value of a random variable X, denoted here as ‚Ñ∞(X), is the arithmetic mean of a large number of outcomes of a random experiment:



	‚Ñ∞(X) = Œ£{x‚ÇôP(x‚Çô) | n ‚àà ‚Ñï}

	if X is discrete. 



	If X is continuous:

	‚Ñ∞(X) = ‚à´ xP(x) dx.



If g(X) is any other random variable that is defined as a function of X:



	‚Ñ∞(g(X)) = Œ£{g(x‚Çô)P(x‚Çô) | n ‚àà ‚Ñï}

	if X is discrete.



	If X is continuous:

	‚Ñ∞(g(X)) = ‚à´ g(x)P(x) dx.



=======  ENTROPY H  =======


:::::::::::::::::::::::EDIT:::::::::::::::::::::::

An alternative formulation of H(X) is constructed from the expectation value of ln(P(X)‚Åª¬π):


	‚Ñ∞(ln(P(X)‚Åª¬π)) = Œ£{ln(P(x‚Çô)‚Åª¬π)P(x‚Çô) | n ‚àà ‚Ñï}.


	ln(P(x‚Çô)‚Åª¬π) = -ln(P(x‚Çô)), so:


	Œ£{ln(P(x‚Çô)‚Åª¬π)P(x‚Çô) | n ‚àà ‚Ñï} = -Œ£{P(x‚Çô)ln(P(x‚Çô)) | n ‚àà ‚Ñï}, thus:


	H(X) = ‚Ñ∞(ln(P(X)‚Åª¬π)).


:::::::::::::::::::::::EDIT:::::::::::::::::::::::




The entropy H(X) of a random variable X is defined as:



	H(X) = -Œ£{P(x‚Çô)ln(P(x‚Çô)) | n ‚àà ‚Ñï} ‚àÄ x‚Çô ‚àà Œ©

	
	if X is discrete. If X is continuous:


	H(X) = -‚à´ P(x)ln(P(x)) dx.



These formulas produce a value of entropy in natural units of information, or nats; if entropy in units of bits is desired, the natural logarithm can be replaced by a base 2 logarithm in the above formulas for both discreet and continuous variables.


Entropy does not depend on the actual values of the random variable x, only on the probability distribution P(x); because values of probability are always nonnegative, values of entropy are also necessarily nonnegative:

	P(x) ‚â• 0 ‚àÄ x ‚àà Œ© ‚áí H(X) ‚â• 0.



=======  JOINT & CONDITIONAL ENTROPY  =======


:::::::::::::::::::::::EDIT:::::::::::::::::::::::


A joint probability distribution such as:

	
	P(X = x, Y = y)


	is the probability that a discrete random variable X takes on the value x and another discrete random variable Y takes on the value y simultaneously. 


Joint entropy of X and Y, H(X, Y), is defined as:


	H(X, Y) = -Œ£Œ£{P(X = x, Y = y)ln(P(X = x, Y = y))} ‚àÄ x ‚àà X, y ‚àà Y.


The conditional entropy, for example H(Y|X = x)...

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Dapibus ultrices in iaculis nunc sed. Fames ac turpis egestas sed tempus urna. Donec et odio pellentesque diam volutpat. Malesuada fames ac turpis egestas sed tempus urna et. Vel risus commodo viverra maecenas accumsan lacus vel facilisis volutpat. Venenatis tellus in metus vulputate eu. Interdum consectetur libero id faucibus nisl tincidunt. Eget magna fermentum iaculis eu non diam phasellus vestibulum. Enim nunc faucibus a pellentesque. Dignissim convallis aenean et tortor at risus viverra adipiscing at. Dignissim enim sit amet venenatis. Sit amet consectetur adipiscing elit ut aliquam purus sit amet. Faucibus interdum posuere lorem ipsum dolor sit amet consectetur.


:::::::::::::::::::::::EDIT:::::::::::::::::::::::

---


EDIT THIS:

The entropy in units of nats of a continuous random variable X with a continuous probability density function P(x), where P(x) is the probability associated with x for any x ‚àà X is:

H(X) = -‚à´P(x)ln(P(x))dx

The expectation value Expectation(X) is:

Expectation(X) = ‚à´ xP(x) dx

The integral is taken over the entire domain of X.



If X is a discreet random variable:

H(X) = -Œ£{P(x‚Çô)ln(P(x‚Çô)) | n ‚àà ‚Ñï}

Expectation(X) = Œ£{x‚ÇôP(x‚Çô) | n ‚àà ‚Ñï}

for each x‚Çô ‚àà X. P(x‚Çô) is the probability associated with the value x‚Çô and ‚Ñï is the set of natural numbers (‚Ñï does not include 0).

H(X) = Expectation( ln(1/P(x‚ààX) )

If entropy in units of bits is desired, the natural log can be replaced by a base 2 logarithm.

Relative entropy is a quantity closely related to metrics like distance between two probability mass functions such as p and q. It is defined as:

D(p||q) =  Œ£‚Çì p(x) log(p(x)/q(x))

There are striking similarities between the calculation of entropy of a random variable with a probability distribution and the calculation of measurables of a quantum system with a probability distribution in the form of a wavefuntion; it is natural to think of the logarithm as an operator on a probability distribution just as other operators act on equations with wavefunctions to calculate measurable variables in a quantum system.

It is clear that logarithms will be relevant in the calculation of entropy of a quantum system; my initial intuition was that the entropy operator that results in a value of entropy with units of nats is simply the natural logarithm (base 2 is be used for units of bits). Further investigation revealed that Von Neumann entropy is typically calculated with quantum systems described in the form of density matrices with a form identical to the definition of entropy above.
